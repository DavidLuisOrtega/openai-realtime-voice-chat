<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Voice Chat</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            background: white;
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .chat-image {
            cursor: pointer;
            transition: transform 0.3s ease;
            transform: scale(0.5);
        }

        .chat-image:hover {
            transform: scale(0.55);
        }

        .chat-image.ai-speaking {
            animation: aiPulse 1.5s ease-in-out infinite;
        }

        .chat-image.user-speaking {
            animation: userPulse 1s ease-in-out infinite;
        }

        @keyframes aiPulse {
            0%, 100% { 
                transform: scale(0.5);
                box-shadow: 0 0 0 0 rgba(0, 123, 255, 0.4);
            }
            50% { 
                transform: scale(0.55);
                box-shadow: 0 0 20px 10px rgba(0, 123, 255, 0.2);
            }
        }

        @keyframes userPulse {
            0%, 100% { 
                transform: scale(0.5);
                box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.4);
            }
            50% { 
                transform: scale(0.53);
                box-shadow: 0 0 15px 8px rgba(40, 167, 69, 0.3);
            }
        }
    </style>
</head>
<body>
    <img src="assets/ChatGPT_cloud.png" alt="Start Chat" class="chat-image" id="chatImage">

    <script>
        class CleanVoiceChat {
            constructor() {
                this.pc = null;
                this.audioEl = null;
                this.isConnected = false;
                this.currentVoice = 'marin';
                this.conversationHistory = [];
                this.maxContextTokens = 8000; // Approximate token limit
                
                this.setupUI();
            }

            setupUI() {
                this.chatImage = document.getElementById('chatImage');
                this.chatImage.onclick = () => {
                    if (this.isConnected) {
                        this.disconnect();
                    } else {
                        this.startVoiceChat();
                    }
                };
                
                // Add double-click to manually add test conversation
                this.chatImage.ondblclick = () => this.addTestConversation();
            }

            setImageState(state) {
                this.chatImage.className = `chat-image ${state}`;
            }

            // Add message to conversation history
            addToHistory(role, content) {
                this.conversationHistory.push({
                    role: role, // 'user' or 'assistant'
                    content: content,
                    timestamp: new Date().toISOString()
                });
                this.trimContextWindow();
            }

            // Trim conversation history to stay within token limit
            trimContextWindow() {
                // Rough token estimation: ~4 characters per token
                let totalTokens = 0;
                let keepMessages = [];
                
                // Count from most recent backwards
                for (let i = this.conversationHistory.length - 1; i >= 0; i--) {
                    const message = this.conversationHistory[i];
                    const messageTokens = Math.ceil(message.content.length / 4);
                    
                    if (totalTokens + messageTokens > this.maxContextTokens) {
                        break; // Stop adding older messages
                    }
                    
                    totalTokens += messageTokens;
                    keepMessages.unshift(message); // Add to beginning
                }
                
                this.conversationHistory = keepMessages;
                console.log(`ðŸ§  Context window: ${keepMessages.length} messages, ~${totalTokens} tokens`);
            }

            // Test method to add sample conversation history
            addTestConversation() {
                this.addToHistory('user', 'Hello, how are you today?');
                this.addToHistory('assistant', 'Hello! I\'m doing well, thank you for asking. How can I help you today?');
                this.addToHistory('user', 'Can you remember what we just talked about?');
                console.log('ðŸ§ª Added test conversation to memory');
            }

            setupConversationTracking() {
                // Note: WebRTC with OpenAI Realtime API handles conversation automatically
                // We'll track it through periodic conversation state checks
                console.log('ðŸ§  Conversation tracking enabled');
                console.log(`ðŸ“Š Current history: ${this.conversationHistory.length} messages`);
            }

            // Disconnect and allow reconnection with memory
            disconnect() {
                if (this.pc) {
                    this.pc.close();
                    this.pc = null;
                }
                
                if (this.audioEl) {
                    this.audioEl.srcObject = null;
                }
                
                this.isConnected = false;
                this.setImageState('');
                console.log('ðŸ”Œ Disconnected - conversation history preserved');
            }

            async startVoiceChat() {
                if (this.isConnected) return;
                
                try {
                    // Set up WebRTC peer connection
                    this.pc = new RTCPeerConnection({
                        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
                    });

                    // Set up audio element for remote audio
                    this.audioEl = document.createElement('audio');
                    this.audioEl.autoplay = true;
                    this.audioEl.volume = 1.0;

                    // Handle incoming audio from OpenAI
                    this.pc.ontrack = (event) => {
                        console.log('ðŸ“¡ Received audio track from OpenAI');
                        this.audioEl.srcObject = event.streams[0];
                        
                        // Monitor AI audio for pulsing
                        const audioContext = new AudioContext();
                        const source = audioContext.createMediaStreamSource(event.streams[0]);
                        const analyser = audioContext.createAnalyser();
                        source.connect(analyser);
                        
                        const checkAudioLevel = () => {
                            const dataArray = new Uint8Array(analyser.frequencyBinCount);
                            analyser.getByteFrequencyData(dataArray);
                            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
                            
                            if (average > 10) {
                                this.setImageState('ai-speaking');
                            } else {
                                this.setImageState('');
                            }
                            
                            if (this.isConnected) {
                                requestAnimationFrame(checkAudioLevel);
                            }
                        };
                        checkAudioLevel();
                    };

                    // Get microphone access
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true
                        }
                    });

                    // Add microphone track to peer connection
                    const micTrack = stream.getAudioTracks()[0];
                    this.pc.addTrack(micTrack, stream);

                    // Monitor user microphone for pulsing
                    const userAudioContext = new AudioContext();
                    const userSource = userAudioContext.createMediaStreamSource(stream);
                    const userAnalyser = userAudioContext.createAnalyser();
                    userSource.connect(userAnalyser);
                    
                    const checkUserAudioLevel = () => {
                        const dataArray = new Uint8Array(userAnalyser.frequencyBinCount);
                        userAnalyser.getByteFrequencyData(dataArray);
                        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
                        
                        if (average > 15) {
                            this.setImageState('user-speaking');
                        } else if (!this.chatImage.classList.contains('ai-speaking')) {
                            this.setImageState('');
                        }
                        
                        if (this.isConnected) {
                            requestAnimationFrame(checkUserAudioLevel);
                        }
                    };
                    checkUserAudioLevel();

                    // Create WebRTC offer
                    const offer = await this.pc.createOffer({ offerToReceiveAudio: true });
                    await this.pc.setLocalDescription(offer);

                    // Send SDP offer to your backend router
                    const response = await fetch('/api/realtime/session', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            sdpOffer: offer.sdp,
                            model: 'gpt-4o-realtime-preview',
                            voice: this.currentVoice
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`SDP exchange failed: ${response.status} ${await response.text()}`);
                    }

                    // Get SDP answer from OpenAI
                    const sdpAnswer = await response.text();
                    await this.pc.setRemoteDescription({
                        type: 'answer',
                        sdp: sdpAnswer
                    });

                    this.isConnected = true;
                    console.log('ðŸŽµ Connected with voice:', this.currentVoice);
                    
                    // Set up conversation tracking via WebRTC data channel
                    this.setupConversationTracking();

                } catch (error) {
                    console.error('Connection error:', error);
                    alert('Failed to connect: ' + error.message);
                }
            }
        }

        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', () => {
            new CleanVoiceChat();
        });
    </script>
</body>
</html>